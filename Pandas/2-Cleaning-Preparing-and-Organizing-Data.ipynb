{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI1ZTE7g4wUw"
      },
      "source": [
        "# Python and Data Analysis 2 - Cleaning, Preparing, and Organizing Data\n",
        "\n",
        "**Goal:** The goal of this project is to learn to prepare data for analysis using Pandas.\n",
        "\n",
        "**Description:** Data often needs to be organized, combined, or cleaned before performing analysis. This project explains a few key ways to accomplish this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyczLpqt4wUx"
      },
      "source": [
        "## 2A: Preparing and Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q__zQolC4wUy"
      },
      "source": [
        "Data *cleaning* refers to the process of ensuring our data is correct and complete. If data is not cleaned, our analysis could produce incorrect results based on corrupt/garbage information. *Preparation* refers to getting our data ready for analysis by putting it into a format we can easily understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZOrnAM-4wUz"
      },
      "source": [
        "### Removing Garbage Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6got9g_4wUz"
      },
      "source": [
        "*Garbage data* depends on the type of data we are representing. One common example is if we have a DataFrame with inconsistent types in its columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZWH7wq4wU0",
        "outputId": "ce28e6d3-ff62-4449-f99c-ac0d4a750ccc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'x': [1,'more garbage',3,4,5,6,7,8,9,10],\n",
        "                  'y': [1,4,9,16,25,36,49,64,'garbage',100]})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MccrLUF4wU6"
      },
      "source": [
        "Clearly, we want `x` and `y` to contain numerical values. Therefore rows 1 and 8 contain garbage data. We can try converting them to numbers to fix this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOmqDYpC4wU7",
        "outputId": "5c59b97e-ba8e-4daa-f47f-c90fefb78999"
      },
      "outputs": [],
      "source": [
        "df = df.apply(pd.to_numeric, errors='coerce') # Go through the DataFrame and force every value to be a number\n",
        "print(df.dtypes)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my68qjHe4wU-"
      },
      "source": [
        "Notice the strings have been replaced `NaN`. This is a special value considered to be a `float`, so it is consistent with the other numerical values in its respective column. However, when NaN is involved in a calculation, it results in NaN output. Therefore, we typically want to remove these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txdoynHn4wU_",
        "outputId": "2082db6f-3744-468b-c1c8-7caa958e50f0"
      },
      "outputs": [],
      "source": [
        "# NaN + 4 = NaN\n",
        "print(df.iloc[1,0] + df.iloc[1,1]) # Add the x value from row 1 (NaN) to the y value from row 1 (4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Xj6ivK4wVC"
      },
      "source": [
        "We can remove `NaN` values in two ways. If we want to completely delete the row containing the NaN, we can use `dropna()`. If we want to replace the NaN with some constant value, we can use `fillna(replacement_value)`. The first example removes rows with NaN, and the second example replaces the NaNs with the value 0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvGDhh444wVC",
        "outputId": "4be3934b-f752-4a79-ea22-54186606c201"
      },
      "outputs": [],
      "source": [
        "df1 = df.dropna()\n",
        "# print(\"Dropping NaNs\")\n",
        "# print(df1)\n",
        "\n",
        "df2 = df.fillna(0)\n",
        "print(\"\\nFilling NaNs\")\n",
        "print(df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8fRYFaY4wVF"
      },
      "source": [
        "### Preparing Data\n",
        "\n",
        "#### Dropping Columns\n",
        "When getting data ready for analysis, we might want to select subsections of our data and crop out the rest. For example, let's say we are interested in the `date`, `close` and `volume` for Microsoft stock. Using the techniques discussed earlier, we can do the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYNdn3Qd4wVF",
        "outputId": "91e0ca51-fc05-4cf1-bfbc-e1ce99c442d3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('MSFT.csv')\n",
        "df = df[['date', 'close', 'volume']]\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAa_H5dO6ZpJ"
      },
      "source": [
        "Alternatively, if there were some columns which we were just not interested in, we could `drop` those and leave the rest. Note: we need `axis=1` to tell Pandas to drop columns and not rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1pmhT-z52rv",
        "outputId": "72896226-2c9c-4a94-c1c9-fdcb4fc12866"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('MSFT.csv')\n",
        "df1 = df1.drop(['close'], axis=1)\n",
        "print(df1.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRiQo324wVI"
      },
      "source": [
        "#### Changing Data Types\n",
        "After collecting the data we want, we sometimes need to change the type of a column. A common example is converting `date` from `string` to `datetime`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBDEIHR44wVJ",
        "outputId": "cf14d537-220d-4ee3-b75c-6236034898bc"
      },
      "outputs": [],
      "source": [
        "df['date'] = pd.to_datetime(df['date']) # Convert 'date' column from string to datetime\n",
        "print(df['date'].head()) # Display the first few rows of the 'date' column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQdI__lW4wVM"
      },
      "source": [
        "#### Renaming Columns\n",
        "We can easily rename our columns with `rename`. We supply a dictionary, mapping the old names to new ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL8oCGBl4wVN",
        "outputId": "fa7f9dc3-5809-4202-d307-09137ecb1c44"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'date': 'Date', 'close': 'Closing Price', 'volume': 'Trading Volume'})\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajg6AiS74wVP"
      },
      "source": [
        "#### Changing Index\n",
        "Finally, we might want to relabel our index, by setting its values to be the `Date` column. This is useful if we frequently want to access rows by `Date`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dIbKUeH4wVQ",
        "outputId": "92863749-d0f4-42ff-a47e-44ca5ed9eb96"
      },
      "outputs": [],
      "source": [
        "df = df.set_index('Date')\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.loc[\"1986-03-17\"])\n",
        "print(df.loc[\"1986-03-13\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bKgpSVP4wVS"
      },
      "source": [
        "Note, the new index is the old `Date` column, which has been removed. We can no longer access `Date` using `df['Date']` because it is no longer considered a column. We would instead access the index using `df.index`, `df.index.values`, or `df.index.values.tolist()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnKao1k97xsP"
      },
      "source": [
        "**Challenge**: Reload the MSFT CSV into a variable called `df1`, then drop the open and volume columns from it, convert the date column to datetime, then use the date column as the index. **Important**: don't name the new dataframe `df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIypAFWv7CUQ",
        "outputId": "4d85aba9-2446-4f68-ab89-e3108d3546c9"
      },
      "outputs": [],
      "source": [
        "df3 = pd.read_csv('MSFT.csv')\n",
        "df3 = df3.drop(['open', 'volume'], axis=1)\n",
        "# print(df3.head())\n",
        "\n",
        "df3['date'] = pd.to_datetime(df3['date'])\n",
        "# print(df3.head())\n",
        "df3 = df3.set_index('date')\n",
        "print(df3.head())\n",
        "\n",
        "print(df3.loc['1986-03-17'])\n",
        "\n",
        "# df1 = pd.read_csv('MSFT.csv')\n",
        "# df1 = df1.drop(['open', 'volume'], axis=1)\n",
        "# df1['date'] = pd.to_datetime(df1['date'])\n",
        "# df1 = df1.set_index('date')\n",
        "# print(df1.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbQonLlj4wVS"
      },
      "source": [
        "### Sorting Data\n",
        "\n",
        "Finally, we might want to sort our data by one or more columns. Lets say we want to sort the data in increasing order by `volume`.\n",
        "\n",
        "We use the syntax `df-name.sort_values()` which takes the parameters:\n",
        " - `by=`: The column name or column list to sort by\n",
        " - `ascending=`: Whether you want to sort in increasing (True) or decreasing (False) order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ABHGhjK4wVT",
        "outputId": "550861e8-e481-4f4a-a7a0-0b3f1d1af26c"
      },
      "outputs": [],
      "source": [
        "msft = pd.read_csv('MSFT.csv')\n",
        "msft = msft.sort_values(by=['volume', 'close'], ascending=True)\n",
        "print(msft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVC4JoxU4wVV"
      },
      "source": [
        "If we wanted to sort by multiple columns, we pass a list into the `by=` parameter. For example, to sort by `volume` and then `close`, we would do `msft.sort_values(by=['volume', 'close'], ascending=True)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kPWMg6M4wVV"
      },
      "source": [
        "## 2B: Combining DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBtTJlkl4wVW"
      },
      "source": [
        "Sometimes, data is scattered across multiple DataFrames. In that case, we need to be able to join DataFrames together. There are several ways to do this in Pandas, but this tutorial will cover `merge`. Suppose we have the following two DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bazynedD4wVW",
        "outputId": "dd9aedf3-ccf4-4070-aa1c-9585ce94bafe"
      },
      "outputs": [],
      "source": [
        "classes = pd.DataFrame({\n",
        "    'class': [1,2,3,4,5],\n",
        "    'teacher': ['Bill Ryan', 'Sue Kim', 'Arun Gupta', 'Darnell Williams', 'Emily Coles'],\n",
        "})\n",
        "\n",
        "teachers = pd.DataFrame({\n",
        "    'teacher': ['Sue Kim', 'Arun Gupta', 'Emily Coles', 'Darnell Williams', 'Ben Smith', 'Nicole Ang'],\n",
        "    'age': [43, 56, 32, 44, 32, 53],\n",
        "    'experience': [4, 5, 2, 6, 1, 4],\n",
        "})\n",
        "\n",
        "print(\"Classes\")\n",
        "print(classes)\n",
        "print(\"\\nTeachers\")\n",
        "print(teachers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJzoF5Zy4wVZ"
      },
      "source": [
        "The `classes` DataFrame contains information about 5 classes and the teachers that teach them. The `teachers` DataFrame contains information about each teacher: their age, and years of experience. We want to obtain one DataFrame, with information about class, teacher, age, and experience. There are four ways to do this. \n",
        "\n",
        "When joining DataFrames, we visualize the operation as adding the *right* DataFrame to the *left* one. We join based on a column that contains the same data between the two DataFrames. In this case, the shared piece of data is the teacher.\n",
        "\n",
        "The syntax is `left_df.merge(right_df, on='', how='')`.\n",
        " - `on=''`: Replace the quotes with the common column between the two DataFrames\n",
        " - `how=''`: Replace the quotes with the type of join: `'left'`, `'right'`, `'outer'`, `'inner'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeOlZNHn4wVa"
      },
      "source": [
        "### Left Join\n",
        "\n",
        "A *left join* keeps all the rows in the left DataFrame, and adds entries from the right DataFrame where the `on` column matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM7-w6PC4wVa",
        "outputId": "e3251b59-6047-45f4-f6a0-3d793912b42d"
      },
      "outputs": [],
      "source": [
        "combined = classes.merge(teachers, on='teacher', how='left')\n",
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px7rHIwH4wVh"
      },
      "source": [
        "The ages and experience for Sue, Arun, Darnell, and Emily have been added to the left DataFrame. Bill Ryan remains because he was in the left DataFrame, but does not have an age or experience because he is not present in the right DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjW-D_iQ4wVi"
      },
      "source": [
        "### Right Join\n",
        "\n",
        "A *right join* keeps all the rows in the right DataFrame, and adds entries from the left DataFrame where the `on` column matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGrsJrOD4wVi",
        "outputId": "7ef585df-28c3-427d-a279-a61b26e39044"
      },
      "outputs": [],
      "source": [
        "combined = classes.merge(teachers, on='teacher', how='right')\n",
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-o10aIt4wVl"
      },
      "source": [
        "As before, the ages and experience for Sue, Arun, Darnell, and Emily have been added. Bill Ryan was not in the right DataFrame, so his name is not present, and his class has disappeared as well. Ben Smith and Nicole Ang were present in the right DataFrame, so they are included, but they do not have an associated class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYNvFg2E4wVl"
      },
      "source": [
        "### Inner Join\n",
        "\n",
        "An *inner join* keeps all the rows where the `on` column matches - in other words, where the `teacher` is present in both DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4xjOA9k4wVl",
        "outputId": "74c02c59-61e5-46b1-8b95-faa39b6f0b3a"
      },
      "outputs": [],
      "source": [
        "combined = classes.merge(teachers, on='teacher', how='inner')\n",
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ophoP50c4wVn"
      },
      "source": [
        "The ages and experience for Sue, Arun, Darnell, and Emily are included because they are present in both DataFrames. Bill Ryan is only present in the left DataFrame, so he is not included. Ben Smith and Nicole Ang are only present in the right DataFrame, so they are not included."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rperCXgY4wVo"
      },
      "source": [
        "### Outer Join\n",
        "\n",
        "An *outer join* keeps all entries from both DataFrames. Where values don't exist, it fills the cell with `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C1eh1hH4wVo",
        "outputId": "5982773d-8e70-4f4f-fbda-2436a0cdcdf9"
      },
      "outputs": [],
      "source": [
        "combined = classes.merge(teachers, on='teacher', how='outer')\n",
        "print(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgdhN62v4wVr"
      },
      "source": [
        "Every teacher is included in this combination. If they don't have an age or experience (Bill Ryan) it is filled with NaN. If they don't have a class (Ben Smith, Nicole Ang), it is filled with NaN. Try and think about which types of joins would be useful in different scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxBYStR_4wVr"
      },
      "source": [
        "## 2C: Grouping Data\n",
        "\n",
        "Finally, if we have a large DataFrame, we might be interested organizing the data into groups. We can do this easily using `groupby`. In the following example, we create dataframes for Microsoft, Apple, Amazon, and Google stock prices. Then we `append` them together into one large DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ8itFLF4wVr",
        "outputId": "706b835a-7230-4be6-8a4b-f56c72911553"
      },
      "outputs": [],
      "source": [
        "stock_names = ['MSFT', 'AAPL', 'AMZN', 'GOOG']\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df_list = []  # create a list for saving DataFrame\n",
        "\n",
        "for stock_name in stock_names:\n",
        "    stock_df = pd.read_csv(f'{stock_name}.csv')  # read CSV\n",
        "    stock_df['name'] = stock_name  # add column with company's name\n",
        "    df_list.append(stock_df)  # add to the list\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True) \n",
        "# ignore_index=True for updating indices\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK2ejCMQ4wVu"
      },
      "source": [
        "Let's say `df` was the only DataFrame we had. Without knowing where each stock's price data begins and ends, there is no way to separate data by stock (the `name` column). Instead, we use `groupby` to create an object that stores the grouped DataFrames. Here, we want to group the rows in the DataFrame by `name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2lFlgSl4wVv",
        "outputId": "73433943-1f53-4edc-952b-d6cad0c65d55"
      },
      "outputs": [],
      "source": [
        "groups = df.groupby('name') # Creates a grouping of the DataFrame by column 'name'\n",
        "print(groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB3BXI464wVx"
      },
      "source": [
        "We cannot display `groups` directly, because it is not a DataFrame. Instead, we can access specific groups stored in `groups`. The following code gets all the rows in the group `'MSFT'`, returns them as a DataFrame, and stores them in the variable `msft_data`. The resulting DataFrame contains price information for only Microsoft's stock."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhApmHoh4wVy",
        "outputId": "8ed51a9e-c0d3-4cbb-e01f-e9c73160f851"
      },
      "outputs": [],
      "source": [
        "msft_data = groups.get_group('AAPL')\n",
        "print(msft_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyva2htE4wV0"
      },
      "source": [
        "To practice, try getting the data for Apple, Google, and Amazon from `groups`. We don't have to group by `name`, but we can also group by other columns as well. For example, if we wanted to compare data across all stocks on a given date, we could `groupby('date')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHj9vTWg4wV1"
      },
      "outputs": [],
      "source": [
        "dates_groups = df.groupby('date') # A grouping of the DataFrame by date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC-79x234wV4"
      },
      "source": [
        "To view the stock prices on a given day (for example January 3, 2018), we can `get_group` for the day we are interested in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt_Yl8qR4wV4",
        "outputId": "c048eb04-396b-4c55-e29c-5cbdaf620278"
      },
      "outputs": [],
      "source": [
        "target_data = dates_groups.get_group('2018-01-03')\n",
        "print(target_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO_UeB9l4wV8"
      },
      "source": [
        "**Challenge**: Find the most common volume amount across all four stocks using `groupby`, then print the rows in the dataframe with that volume using `get_group`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaJ57C1O4wV8",
        "outputId": "8731deab-28ee-4114-91de-b86be598b178"
      },
      "outputs": [],
      "source": [
        "t = df.groupby('volume')\n",
        "# print(t.get_group(7900))\n",
        "# print(t.groups.items())\n",
        "# Sort based on the number of values in each group\n",
        "most_common = max(t.groups.items(), key=lambda item: len(item[1]))\n",
        "# print(most_common)\n",
        "# print(t.get_group(most_common[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2-Cleaning-Preparing-and-Organizing-Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
